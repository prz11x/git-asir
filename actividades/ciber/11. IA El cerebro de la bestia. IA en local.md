## Requisitos mínimos Ollama (realistas)

Antes de tocar nada, conviene saber qué espera Ollama del hierro:

- Ubuntu Server 24.04 LTS (OK)
- Arquitectura x86_64
- CPU decente (funciona sin GPU, pero paciencia)
- RAM recomendada:
    - 8 GB → modelos pequeños (phi, tinyllama)
    - 16 GB o más → llama3, mistral, etc.
- Acceso a internet

Comprueba lo básico:

`rui@rui-VMware-Virtual-Platform:~$ lsb_release -a`
`No LSB modules are available.`
`Distributor ID:	Ubuntu`
`Description:	Ubuntu 24.04.3 LTS`
`Release:	24.04`
`Codename:	noble`
`rui@rui-VMware-Virtual-Platform:~$ uname -m`
`x86_64`
`rui@rui-VMware-Virtual-Platform:~$ free -h`
               `total       usado       libre  compartido   búf/caché  disponible`
`Mem:           3,8Gi       1,6Gi       1,3Gi        40Mi       1,1Gi       2,1Gi`
`Inter:         3,8Gi          0B       3,8Gi`

# Instalación

Ollama se instala con **un único script oficial**.

curl -fsSL https://ollama.com/install.sh | sh

### Comprobar que el servicio está vivo

En un server, **systemd manda**:

```
systemctl status ollama
```

Deberías ver algo como:

- `Active: active (running)`
- Escuchando en `127.0.0.1:11434`

rui@rui-VMware-Virtual-Platform:~$ systemctl status ollama
● ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled)
     Active: active (running) since Thu 2026-01-08 12:34:34 CET; 46s ago
     Main PID: 18970 (ollama)
      Tasks: 9 (limit: 4543)
     Memory: 35.2M (peak: 45.4M)
        CPU: 120ms
     CGroup: /system.slice/ollama.service
             └─18970 /usr/local/bin/ollama serve


Si no está activo:

```
sudo systemctl enable --now ollama
```



### Probar Ollama desde terminal

Aquí viene la parte bonita: **usar IA sin navegador**.

Prueba con un modelo ligero primero:

```
ollama run phi
```

O uno más conocido:

```
ollama run llama3
```

La primera vez:

- Descarga el modelo
- Puede tardar (no está roto, está pensando)

Cuando veas un prompt tipo:

```
>>> 
```

Ya tienes un LLM local funcionando. Sin nube. Sin vender tu alma.

---

### Ver modelos instalados

```
ollama list
```

\Users\pablo>ollama list
NAME                  ID              SIZE      MODIFIED
deepseek-r1:latest    6995872bfe4c    5.2 GB    2 months ago
deepseek-r1:8b        6995872bfe4c    5.2 GB    2 months ago

Descargar modelos sin ejecutarlos:

```
ollama pull mistral
ollama pull llama3:8b
```

---

### Uso como servicio (API local)

Ollama levanta una **API REST** en local:

- URL: `http://localhost:11434`

Prueba rápida:

```
curl http://localhost:11434/api/tags
```

Esto es oro puro para:

- Scripts Python
- Herramientas OSINT
- Automatización en ciber
- Integrarlo con apps web internas

### Permitir acceso desde otra máquina (opcional)

⚠️ **Ojo sysadmin**: por defecto SOLO escucha en localhost.

Si quieres que otros equipos accedan (por ejemplo, alumnos o un frontend):

Edita el servicio:

```
sudo systemctl edit ollama
```

Añade:

```
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
```

Luego:

```
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

Y abre firewall si procede:

```
sudo ufw allow 11434/tcp
```

---

### Dónde guarda Ollama los modelos

Importante para servidores con discos separados:

- Por defecto:  
    `/usr/share/ollama/.ollama`

Puedes moverlo usando:

```
export OLLAMA_MODELS=/ruta/grande/ollama-models
```

En server con `/data` o `/mnt`, esto es muy recomendable.

---

## Desinstalar

```
sudo systemctl stop ollama
sudo rm /usr/bin/ollama
sudo rm /etc/systemd/system/ollama.service
sudo rm -rf /usr/share/ollama
```


### Comprobar que Ollama está instalado y activo

En tu **Ubuntu Server 24**:

```
ollama --version
systemctl status ollama
```

Si ves versión y estado `active (running)`, seguimos.

rui@rui-VMware-Virtual-Platform:~$ ollama --version
ollama version is 0.13.5

---

### Instalar (descargar) el modelo `codellama`

Instalar un modelo en Ollama es simplemente **hacer un pull**:

```
ollama pull codellama
```



Qué ocurre aquí:

- Se descarga el modelo
- Se guarda en disco
- No se ejecuta todavía
- Puede tardar (son ~4 GB)

Paciencia de sysadmin.

---

### Verificar que el modelo está instalado

```
ollama list
```

Deberías ver algo así:

```
NAME           ID            SIZE     MODIFIED
codellama      xxxx          ~4.0 GB  …
```

rui@rui-VMware-Virtual-Platform:~$ ollama list
NAME              ID              SIZE      MODIFIED           
mistral:latest    6577803aa9a0    4.4 GB    About a minute ago
---

### Usar el modelo

### Modo interactivo

```
ollama run mistral
```

Te aparecerá:

```
>>>
```

Ejemplo:

```
>>> escribe un script bash que liste usuarios conectados
```

Responderá orientado a código, no a charla.

---

### Uso directo (una sola petición)

```
ollama run codellama "escribe un script en bash para hacer backup de /etc"
```

C:\Users\pablo>ollama run deepseek-r1:latest
⠼ ^C
C:\Users\pablo>ollama run deepseek-r1:latest
>>> escribe un script bash que liste usuarios conectados
Thinking...
Perfecto para redirecciones:

```
ollama run codellama "crea un script de backup de /etc" > backup.sh
chmod +x backup.sh
```

Ollama puede generar contenido:

```
ollama run llama3 "Escribe un README.md sobre este proyecto" > README.md
```

Aquí **sí crea archivos**, pero porque **la shell redirige la salida**.  
Ollama sigue sin tocar el sistema.

### **El peligro real**

El riesgo no es Ollama.  
El riesgo es esto:

**#Peligro**  
`eval "$(ollama run llama3 'dame un comando para limpiar el sistema')"`

### Ejecutar comandos con control (wrapper en Bash)

```
#!/bin/bash
if [ $# -gt 0 ]; then
  pregunta$1
else
    read -p "Pregunta a la IA: " pregunta
fi



respuesta=$(ollama run mistral "$pregunta. Devuelve SOLO un comando seguro.")

echo "La IA propone ejecutar:"
echo "$respuesta"

read -p "¿Ejecutar? (s/n): " ok
if [[ "$ok" == "s" ]]; then
    eval "$respuesta"
fi
```

Qué ocurre aquí:

- Ollama **propone**
- El humano **autoriza**
- El sistema **ejecuta**

En Linux, un comando es solo:

- un archivo ejecutable
- ubicado en un directorio que esté en `$PATH`

Si eso se cumple → se ejecuta desde cualquier sitio.

Compruébalo:

```
echo $PATH
```

Verás cosas como:

- `/usr/bin`
- `/usr/local/bin`
- `/bin`

Ahí es donde viven los comandos “del sistema”.

---

### Opción recomendada: `/usr/local/bin` (limpio y correcto)

Supongamos que tu script se llama `ollama-shell.sh`.

### **Mover el script**

```
sudo mv ollama-shell.sh /usr/local/bin/ollama-shell
```

Nota:

- Quitamos `.sh` → parece un comando real
- Linux no necesita extensión

---

### **Dar permisos de ejecución**

```
sudo chmod +x /usr/local/bin/ollama-shell
```

---

### **Probar desde cualquier sitio**

```
cd /tmp
ollama-shell
```

## Script Wraper avanzado

Aquí tienes un script con muchas mas opciones:

```
#!/usr/bin/env bash
set -euo pipefail

# -----------------------------
# Ollama Shell Assistant (Seguro)
# - Pregunta a un modelo Ollama local
# - Fuerza español usando modelo "llama3-es"
# - Devuelve un único comando
# - Valida contra lista blanca
# - Pide confirmación antes de ejecutar
# - Registra todo en /var/log/ollama-shell.log
# -----------------------------

MODEL="${MODEL:-llama3-es}"
LOG_FILE="${LOG_FILE:-/var/log/ollama-shell.log}"

MODE="${MODE:-ask}"  # ask | exec
TIMEOUT_SECS="${TIMEOUT_SECS:-120}"

# Lista blanca básica de comandos permitidos (ajústala a tu contexto)
ALLOW_CMDS=(
  "ls" "ll" "pwd" "cd"
  "cat" "less" "head" "tail"
  "grep" "egrep" "fgrep" "awk" "sed" "cut" "sort" "uniq" "wc"
  "find" "locate" "which" "whereis"
  "ip" "ss" "netstat" "ping" "traceroute" "dig" "nslookup" "curl" "wget"
  "ps" "top" "htop" "free" "df" "du" "uptime" "uname" "lsblk"
  "systemctl" "journalctl"
  "who" "w" "id" "groups" "last" "lastb"
  "tar" "gzip" "gunzip" "zip" "unzip"
  "apt" "apt-get" "dpkg" "snap"
  "nmap"
)

print_help() {
  cat <<'EOF'
Uso:
  ollama-shell [-e] [-m MODELO] "pregunta..."
  ollama-shell --suggest "pregunta..."
  ollama-shell --exec "pregunta..."
  ollama-shell --help

Opciones:
  -m, --model     Modelo Ollama a usar (por defecto: llama3-es)
  -e, --exec      Permite ejecutar (con confirmación humana)
  --suggest       Solo sugiere (no ejecuta)
  --log FILE      Archivo de log (por defecto: /var/log/ollama-shell.log)

Variables:
  MODEL=llama3-es
  MODE=ask|exec
  LOG_FILE=/var/log/ollama-shell.log
  TIMEOUT_SECS=120

Notas:
- El modelo debe devolver SOLO UN comando.
- Se valida el comando contra una lista blanca básica.
EOF
}

die() {
  echo "ERROR: $*" >&2
  exit 1
}

ensure_ollama() {
  command -v ollama >/dev/null 2>&1 || die "No encuentro 'ollama' en PATH. ¿Está instalado?"
  systemctl is-active --quiet ollama 2>/dev/null || true
}

ensure_log_writable() {
  # Intentar escribir en LOG_FILE; si no se puede, degradar a ~/.ollama-shell.log
  if ! (touch "$LOG_FILE" 2>/dev/null); then
    LOG_FILE="$HOME/.ollama-shell.log"
    touch "$LOG_FILE" || die "No puedo escribir logs ni en /var/log ni en HOME."
  fi
}

log_line() {
  local msg="$1"
  printf '%s | user=%s | cwd=%s | %s\n' "$(date '+%F %T')" "${USER:-unknown}" "$(pwd)" "$msg" >> "$LOG_FILE"
}

trim() {
  sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//'
}

first_token() {
  # Devuelve el primer token (comando base), ignorando sudo si aparece
  local s="$1"
  s="$(echo "$s" | trim)"
  if [[ "$s" == sudo\ * ]]; then
    echo "$s" | awk '{print $2}'
  else
    echo "$s" | awk '{print $1}'
  fi
}

is_allowed_cmd() {
  local cmd="$1"
  for a in "${ALLOW_CMDS[@]}"; do
    [[ "$cmd" == "$a" ]] && return 0
  done
  return 1
}

looks_dangerous() {
  # Filtros anti-destrucción básicos (no exhaustivos)
  local s="$1"
  s="$(echo "$s" | trim)"
  [[ -z "$s" ]] && return 0

  # Cosas típicas peligrosas
  if echo "$s" | grep -Eqi 'rm\s+-rf\s+/' ; then return 0; fi
  if echo "$s" | grep -Eqi 'mkfs|dd\s+if=|:\(\)\s*\{\s*:\|\s*:\s*;\s*\}\s*;|shutdown|reboot|poweroff' ; then return 0; fi
  if echo "$s" | grep -Eqi '>\s*/etc/|>\s*/bin/|>\s*/usr/bin/|chown\s+root|chmod\s+4[0-7]{3}|chmod\s+u\+s' ; then return 0; fi

  # Inyección obvia
  if echo "$s" | grep -Eqi '\$\(|`' ; then return 0; fi

  return 1
}

ask_ollama_for_command() {
  local question="$1"

  local prompt
  prompt=$(
    cat <<EOF
Eres un asistente técnico experto en Linux y ciberseguridad.
Respondes SIEMPRE en español.
Tu tarea: proponer EXACTAMENTE UN comando de terminal (una sola línea) para ayudar con la petición del usuario.
REGLAS:
- Devuelve SOLO el comando. Sin explicaciones. Sin comillas. Sin markdown.
- El comando debe ser lo más seguro posible (preferir lectura/consulta a escritura).
- Evita acciones destructivas. No uses rm, mkfs, dd, ni cambios de permisos peligrosos.
Petición del usuario:
$question
EOF
  )

  # Timeout para evitar bloqueos largos
  # Nota: 'timeout' suele estar en coreutils
  if command -v timeout >/dev/null 2>&1; then
    timeout "$TIMEOUT_SECS" ollama run "$MODEL" "$prompt" 2>/dev/null | head -n 1 | trim
  else
    ollama run "$MODEL" "$prompt" 2>/dev/null | head -n 1 | trim
  fi
}

confirm() {
  local msg="$1"
  read -r -p "$msg (s/n): " ans
  [[ "${ans,,}" == "s" ]]
}

main() {
  local question=""
  local explicit_mode=""
  local explicit_model=""
  local explicit_log=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      -m|--model) explicit_model="$2"; shift 2 ;;
      -e|--exec) explicit_mode="exec"; shift ;;
      --suggest) explicit_mode="ask"; shift ;;
      --log) explicit_log="$2"; shift 2 ;;
      -h|--help) print_help; exit 0 ;;
      *)
        # Resto como pregunta (permitimos comillas)
        question="$*"
        break
        ;;
    esac
  done

  [[ -n "$explicit_model" ]] && MODEL="$explicit_model"
  [[ -n "$explicit_mode" ]] && MODE="$explicit_mode"
  [[ -n "$explicit_log" ]] && LOG_FILE="$explicit_log"

  [[ -z "$question" ]] && die "Falta la pregunta. Usa --help para ver ejemplos."

  ensure_ollama
  ensure_log_writable

  log_line "question=$(printf '%q' "$question") model=$MODEL mode=$MODE"

  local cmd
  cmd="$(ask_ollama_for_command "$question")"

  log_line "raw_cmd=$(printf '%q' "$cmd")"

  [[ -z "$cmd" ]] && die "El modelo no devolvió un comando."

  # Validaciones básicas
  if looks_dangerous "$cmd"; then
    log_line "blocked_reason=dangerous_pattern"
    die "Comando bloqueado por patrón peligroso: $cmd"
  fi

  local base
  base="$(first_token "$cmd")"
  if ! is_allowed_cmd "$base"; then
    log_line "blocked_reason=not_in_allowlist base=$base"
    die "Comando bloqueado (no está en la lista blanca): $base"
  fi

  echo "$cmd"

  if [[ "$MODE" == "exec" ]]; then
    if confirm "¿Ejecutar el comando anterior?"; then
      log_line "exec=YES cmd=$(printf '%q' "$cmd")"
      # Ejecuta tal cual (sin eval). Bash interpretará la línea como comando.
      bash -lc "$cmd"
    else
      log_line "exec=NO"
      echo "Cancelado."
    fi
  else
    echo "Modo sugerencia (no ejecuto nada). Usa --exec para permitir ejecución con confirmación."
  fi
}

main "$@"
```

---

### Instalación global (para todo el sistema)

1. Mueve el script a `/usr/local/bin` y dale permisos:

```
sudo mv ollama-shell /usr/local/bin/ollama-shell
sudo chmod +x /usr/local/bin/ollama-shell
```

2. Prueba:

```
ollama-shell "muestra los puertos en escucha"
```

3. Para permitir ejecución (con confirmación):

```
ollama-shell --exec "muestra los últimos intentos de login fallidos"
```
